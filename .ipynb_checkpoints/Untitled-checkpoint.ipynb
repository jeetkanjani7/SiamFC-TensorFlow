{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False\n",
    "trainable = True\n",
    "embed_config = dict()\n",
    "\n",
    "inputs = tf.get_variable(\"input\", shape = [64, 255,255, 3])\n",
    "reuse=None\n",
    "scope='convolutional_alexnet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(scope, 'convolutional_alexnet', [inputs], reuse=reuse) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "        net = inputs\n",
    "        net = slim.conv2d(net, 96, [11, 11], 2, scope='conv1')\n",
    "        net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\n",
    "        with tf.variable_scope('conv2'):\n",
    "            b1, b2 = tf.split(net, 2, 3)\n",
    "            b1 = slim.conv2d(b1, 128, [5, 5], scope='b1')\n",
    "            # The original implementation has bias terms for all convolution, but\n",
    "            # it actually isn't necessary if the convolution layer is followed by a batch\n",
    "            # normalization layer since batch norm will subtract the mean.\n",
    "            b2 = slim.conv2d(b2, 128, [5, 5], scope='b2')\n",
    "            net = tf.concat([b1, b2], 3)\n",
    "        net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n",
    "        net = slim.conv2d(net, 384, [3, 3], 1, scope='conv3')\n",
    "        with tf.variable_scope('conv4'):\n",
    "            b1, b2 = tf.split(net, 2, 3)\n",
    "            b1 = slim.conv2d(b1, 192, [3, 3], 1, scope='b1')\n",
    "            b2 = slim.conv2d(b2, 192, [3, 3], 1, scope='b2')\n",
    "            net = tf.concat([b1, b2], 3)\n",
    "          # Conv 5 with only convolution, has bias\n",
    "        with tf.variable_scope('conv5'):\n",
    "            with slim.arg_scope([slim.conv2d],\n",
    "                            activation_fn=None, normalizer_fn=None):\n",
    "                b1, b2 = tf.split(net, 2, 3)\n",
    "                b1 = slim.conv2d(b1, 128, [3, 3], 1, scope='b1')\n",
    "                b2 = slim.conv2d(b2, 128, [3, 3], 1, scope='b2')\n",
    "                net = tf.concat([b1, b2], 3)\n",
    "        with tf.variable_scope('lstm'):\n",
    "            net = tf.reshape(net, [64, -1, 256])\n",
    "            tf.transpose(net, [0,2,1])\n",
    "            cell = LSTMCell(128, state_is_tuple=True)\n",
    "            cell1 = LSTMCell(128, state_is_tuple=True)\n",
    "            stack = tf.contrib.rnn.MultiRNNCell([cell, cell1], state_is_tuple= True)\n",
    "            output, states = tf.nn.dynamic_rnn(stack, net,X,dtype=tf.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Reshapes a tensor.\n",
       "\n",
       "Given `tensor`, this operation returns a tensor that has the same values\n",
       "as `tensor` with shape `shape`.\n",
       "\n",
       "If one component of `shape` is the special value -1, the size of that dimension\n",
       "is computed so that the total size remains constant.  In particular, a `shape`\n",
       "of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.\n",
       "\n",
       "If `shape` is 1-D or higher, then the operation returns a tensor with shape\n",
       "`shape` filled with the values of `tensor`. In this case, the number of elements\n",
       "implied by `shape` must be the same as the number of elements in `tensor`.\n",
       "\n",
       "For example:\n",
       "\n",
       "```\n",
       "# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "# tensor 't' has shape [9]\n",
       "reshape(t, [3, 3]) ==> [[1, 2, 3],\n",
       "                        [4, 5, 6],\n",
       "                        [7, 8, 9]]\n",
       "\n",
       "# tensor 't' is [[[1, 1], [2, 2]],\n",
       "#                [[3, 3], [4, 4]]]\n",
       "# tensor 't' has shape [2, 2, 2]\n",
       "reshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n",
       "                        [3, 3, 4, 4]]\n",
       "\n",
       "# tensor 't' is [[[1, 1, 1],\n",
       "#                 [2, 2, 2]],\n",
       "#                [[3, 3, 3],\n",
       "#                 [4, 4, 4]],\n",
       "#                [[5, 5, 5],\n",
       "#                 [6, 6, 6]]]\n",
       "# tensor 't' has shape [3, 2, 3]\n",
       "# pass '[-1]' to flatten 't'\n",
       "reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
       "\n",
       "# -1 can also be used to infer the shape\n",
       "\n",
       "# -1 is inferred to be 9:\n",
       "reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
       "# -1 is inferred to be 2:\n",
       "reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
       "# -1 is inferred to be 3:\n",
       "reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
       "                              [2, 2, 2],\n",
       "                              [3, 3, 3]],\n",
       "                             [[4, 4, 4],\n",
       "                              [5, 5, 5],\n",
       "                              [6, 6, 6]]]\n",
       "\n",
       "# tensor 't' is [7]\n",
       "# shape `[]` reshapes to a scalar\n",
       "reshape(t, []) ==> 7\n",
       "```\n",
       "\n",
       "Args:\n",
       "  tensor: A `Tensor`.\n",
       "  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
       "    Defines the shape of the output tensor.\n",
       "  name: A name for the operation (optional).\n",
       "\n",
       "Returns:\n",
       "  A `Tensor`. Has the same type as `tensor`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  7  8  9]\n",
      " [ 4  5  6 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_peepholes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unit_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proj_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Long short-term memory unit (LSTM) recurrent network cell.\n",
       "\n",
       "The default non-peephole implementation is based on:\n",
       "\n",
       "  http://www.bioinf.jku.at/publications/older/2604.pdf\n",
       "\n",
       "S. Hochreiter and J. Schmidhuber.\n",
       "\"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.\n",
       "\n",
       "The peephole implementation is based on:\n",
       "\n",
       "  https://research.google.com/pubs/archive/43905.pdf\n",
       "\n",
       "Hasim Sak, Andrew Senior, and Francoise Beaufays.\n",
       "\"Long short-term memory recurrent neural network architectures for\n",
       " large scale acoustic modeling.\" INTERSPEECH, 2014.\n",
       "\n",
       "The class uses optional peep-hole connections, optional cell clipping, and\n",
       "an optional projection layer.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Initialize the parameters for an LSTM cell.\n",
       "\n",
       "Args:\n",
       "  num_units: int, The number of units in the LSTM cell.\n",
       "  use_peepholes: bool, set True to enable diagonal/peephole connections.\n",
       "  cell_clip: (optional) A float value, if provided the cell state is clipped\n",
       "    by this value prior to the cell output activation.\n",
       "  initializer: (optional) The initializer to use for the weight and\n",
       "    projection matrices.\n",
       "  num_proj: (optional) int, The output dimensionality for the projection\n",
       "    matrices.  If None, no projection is performed.\n",
       "  proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n",
       "    provided, then the projected values are clipped elementwise to within\n",
       "    `[-proj_clip, proj_clip]`.\n",
       "  num_unit_shards: Deprecated, will be removed by Jan. 2017.\n",
       "    Use a variable_scope partitioner instead.\n",
       "  num_proj_shards: Deprecated, will be removed by Jan. 2017.\n",
       "    Use a variable_scope partitioner instead.\n",
       "  forget_bias: Biases of the forget gate are initialized by default to 1\n",
       "    in order to reduce the scale of forgetting at the beginning of\n",
       "    the training. Must set it manually to `0.0` when restoring from\n",
       "    CudnnLSTM trained checkpoints.\n",
       "  state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
       "    the `c_state` and `m_state`.  If False, they are concatenated\n",
       "    along the column axis.  This latter behavior will soon be deprecated.\n",
       "  activation: Activation function of the inner states.  Default: `tanh`.\n",
       "  reuse: (optional) Python boolean describing whether to reuse variables\n",
       "    in an existing scope.  If not `True`, and the existing scope already has\n",
       "    the given variables, an error is raised.\n",
       "  name: String, the name of the layer. Layers with the same name will\n",
       "    share weights, but to avoid mistakes we require reuse=True in such\n",
       "    cases.\n",
       "\n",
       "  When restoring from CudnnLSTM-trained checkpoints, use\n",
       "  `CudnnCompatibleLSTMCell` instead.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.contrib.rnn.LSTMCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
