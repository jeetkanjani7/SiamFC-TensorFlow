INFO - SiamFC-3s-gray-scratch - Running command 'main'
INFO - SiamFC-3s-gray-scratch - Started run with ID "2"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
INFO - root - preproces -- siamese_fc_gray
INFO - root - embedding init method -- kaiming_normal
INFO - root - preproces -- None
2018-03-12 23:33:28.203148: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-12 23:33:28.318571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-12 23:33:28.318969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.37GiB
2018-03-12 23:33:28.318998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO - root - Train for 332500 steps
INFO - root - 2018-03-12 23:33:37.970207: step 0, total loss = 3.03, batch loss = 2.47 (0.9 examples/sec; 9.224 sec/batch; 851h:54m:29s remains)
INFO:tensorflow:Logs/SiamFC/track_model_checkpoints/SiamFC-3s-gray-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - Logs/SiamFC/track_model_checkpoints/SiamFC-3s-gray-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2018-03-12 23:33:41.713877: step 10, total loss = 1.11, batch loss = 0.55 (33.2 examples/sec; 0.241 sec/batch; 22h:16m:36s remains)
INFO - root - 2018-03-12 23:33:43.913815: step 20, total loss = 0.82, batch loss = 0.26 (36.3 examples/sec; 0.220 sec/batch; 20h:21m:48s remains)
INFO - root - 2018-03-12 23:33:46.057218: step 30, total loss = 0.77, batch loss = 0.20 (36.6 examples/sec; 0.218 sec/batch; 20h:10m:36s remains)
INFO - root - 2018-03-12 23:33:48.224978: step 40, total loss = 0.70, batch loss = 0.13 (33.7 examples/sec; 0.237 sec/batch; 21h:53m:54s remains)
INFO - root - 2018-03-12 23:33:50.622819: step 50, total loss = 0.67, batch loss = 0.11 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:04s remains)
INFO - root - 2018-03-12 23:33:52.822955: step 60, total loss = 0.69, batch loss = 0.13 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:10s remains)
INFO - root - 2018-03-12 23:33:55.026037: step 70, total loss = 0.67, batch loss = 0.11 (35.0 examples/sec; 0.229 sec/batch; 21h:07m:03s remains)
INFO - root - 2018-03-12 23:33:57.205760: step 80, total loss = 0.69, batch loss = 0.13 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:18s remains)
INFO - root - 2018-03-12 23:33:59.319525: step 90, total loss = 0.65, batch loss = 0.09 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:59s remains)
INFO - root - 2018-03-12 23:34:01.532404: step 100, total loss = 0.65, batch loss = 0.09 (37.1 examples/sec; 0.215 sec/batch; 19h:53m:41s remains)
INFO - root - 2018-03-12 23:34:04.051803: step 110, total loss = 0.65, batch loss = 0.09 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:49s remains)
INFO - root - 2018-03-12 23:34:06.232695: step 120, total loss = 0.64, batch loss = 0.08 (38.6 examples/sec; 0.207 sec/batch; 19h:08m:36s remains)
INFO - root - 2018-03-12 23:34:08.381951: step 130, total loss = 0.64, batch loss = 0.08 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:58s remains)
INFO - root - 2018-03-12 23:34:10.522384: step 140, total loss = 0.63, batch loss = 0.07 (36.5 examples/sec; 0.219 sec/batch; 20h:15m:36s remains)
INFO - root - 2018-03-12 23:34:12.644058: step 150, total loss = 0.65, batch loss = 0.09 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:31s remains)
INFO - root - 2018-03-12 23:34:14.749947: step 160, total loss = 0.64, batch loss = 0.08 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:29s remains)
INFO - root - 2018-03-12 23:34:16.890346: step 170, total loss = 0.63, batch loss = 0.07 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:00s remains)
INFO - root - 2018-03-12 23:34:19.010404: step 180, total loss = 0.63, batch loss = 0.07 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:48s remains)
INFO - root - 2018-03-12 23:34:21.125558: step 190, total loss = 0.63, batch loss = 0.07 (35.3 examples/sec; 0.227 sec/batch; 20h:56m:57s remains)
INFO - root - 2018-03-12 23:34:23.260541: step 200, total loss = 0.63, batch loss = 0.08 (39.7 examples/sec; 0.201 sec/batch; 18h:35m:18s remains)
INFO - root - 2018-03-12 23:34:25.794276: step 210, total loss = 0.65, batch loss = 0.09 (37.1 examples/sec; 0.216 sec/batch; 19h:55m:25s remains)
INFO - root - 2018-03-12 23:34:28.009508: step 220, total loss = 0.67, batch loss = 0.11 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:57s remains)
INFO - root - 2018-03-12 23:34:30.137662: step 230, total loss = 0.68, batch loss = 0.12 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:09s remains)
INFO - root - 2018-03-12 23:34:32.296327: step 240, total loss = 0.65, batch loss = 0.09 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:22s remains)
INFO - root - 2018-03-12 23:34:34.427786: step 250, total loss = 0.63, batch loss = 0.07 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:00s remains)
INFO - root - 2018-03-12 23:34:36.576278: step 260, total loss = 0.62, batch loss = 0.07 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:17s remains)
INFO - root - 2018-03-12 23:34:38.692709: step 270, total loss = 0.65, batch loss = 0.10 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:38s remains)
INFO - root - 2018-03-12 23:34:40.801649: step 280, total loss = 0.62, batch loss = 0.07 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:02s remains)
INFO - root - 2018-03-12 23:34:42.930024: step 290, total loss = 0.62, batch loss = 0.07 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:36s remains)
INFO - root - 2018-03-12 23:34:45.042857: step 300, total loss = 0.62, batch loss = 0.07 (40.9 examples/sec; 0.195 sec/batch; 18h:02m:22s remains)
INFO - root - 2018-03-12 23:34:48.280563: step 310, total loss = 0.62, batch loss = 0.07 (34.9 examples/sec; 0.229 sec/batch; 21h:07m:59s remains)
INFO - root - 2018-03-12 23:34:50.515678: step 320, total loss = 0.63, batch loss = 0.08 (33.4 examples/sec; 0.240 sec/batch; 22h:06m:43s remains)
INFO - root - 2018-03-12 23:34:52.621747: step 330, total loss = 0.62, batch loss = 0.07 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:52s remains)
INFO - root - 2018-03-12 23:34:54.764145: step 340, total loss = 0.61, batch loss = 0.06 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:43s remains)
INFO - root - 2018-03-12 23:34:56.909616: step 350, total loss = 0.61, batch loss = 0.06 (39.8 examples/sec; 0.201 sec/batch; 18h:32m:43s remains)
INFO - root - 2018-03-12 23:34:59.068453: step 360, total loss = 0.62, batch loss = 0.07 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:45s remains)
INFO - root - 2018-03-12 23:35:01.271807: step 370, total loss = 0.64, batch loss = 0.09 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:23s remains)
INFO - root - 2018-03-12 23:35:03.445861: step 380, total loss = 0.61, batch loss = 0.06 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:44s remains)
INFO - root - 2018-03-12 23:35:05.565842: step 390, total loss = 0.60, batch loss = 0.06 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:23s remains)
INFO - root - 2018-03-12 23:35:07.692925: step 400, total loss = 0.60, batch loss = 0.05 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:16s remains)
INFO - root - 2018-03-12 23:35:10.145429: step 410, total loss = 0.60, batch loss = 0.06 (34.9 examples/sec; 0.229 sec/batch; 21h:07m:51s remains)
INFO - root - 2018-03-12 23:35:12.279972: step 420, total loss = 0.59, batch loss = 0.05 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:11s remains)
INFO - root - 2018-03-12 23:35:14.477744: step 430, total loss = 0.59, batch loss = 0.05 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:30s remains)
INFO - root - 2018-03-12 23:35:16.589964: step 440, total loss = 0.59, batch loss = 0.05 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:11s remains)
INFO - root - 2018-03-12 23:35:18.712419: step 450, total loss = 0.60, batch loss = 0.06 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:00s remains)
INFO - root - 2018-03-12 23:35:20.883184: step 460, total loss = 0.60, batch loss = 0.06 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:11s remains)
INFO - root - 2018-03-12 23:35:22.986246: step 470, total loss = 0.59, batch loss = 0.05 (37.5 examples/sec; 0.214 sec/batch; 19h:42m:06s remains)
INFO - root - 2018-03-12 23:35:25.107608: step 480, total loss = 0.59, batch loss = 0.05 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:45s remains)
INFO - root - 2018-03-12 23:35:27.263940: step 490, total loss = 0.62, batch loss = 0.08 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:10s remains)
INFO - root - 2018-03-12 23:35:29.406080: step 500, total loss = 0.62, batch loss = 0.08 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:52s remains)
INFO - root - 2018-03-12 23:35:31.798914: step 510, total loss = 0.61, batch loss = 0.07 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:25s remains)
INFO - root - 2018-03-12 23:35:33.961845: step 520, total loss = 0.59, batch loss = 0.05 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:30s remains)
INFO - root - 2018-03-12 23:35:36.147546: step 530, total loss = 0.59, batch loss = 0.05 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:54s remains)
INFO - root - 2018-03-12 23:35:38.242474: step 540, total loss = 0.59, batch loss = 0.05 (38.7 examples/sec; 0.207 sec/batch; 19h:03m:57s remains)
INFO - root - 2018-03-12 23:35:40.397234: step 550, total loss = 0.59, batch loss = 0.06 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:24s remains)
INFO - root - 2018-03-12 23:35:42.568977: step 560, total loss = 0.59, batch loss = 0.06 (34.4 examples/sec; 0.233 sec/batch; 21h:27m:41s remains)
INFO - root - 2018-03-12 23:35:44.725092: step 570, total loss = 0.60, batch loss = 0.06 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:59s remains)
INFO - root - 2018-03-12 23:35:46.816229: step 580, total loss = 0.59, batch loss = 0.05 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:12s remains)
INFO - root - 2018-03-12 23:35:48.949460: step 590, total loss = 0.58, batch loss = 0.05 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:07s remains)
INFO - root - 2018-03-12 23:35:51.098197: step 600, total loss = 0.59, batch loss = 0.06 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:19s remains)
INFO - root - 2018-03-12 23:35:53.605164: step 610, total loss = 0.59, batch loss = 0.05 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:24s remains)
INFO - root - 2018-03-12 23:35:55.714334: step 620, total loss = 0.57, batch loss = 0.04 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:19s remains)
INFO - root - 2018-03-12 23:35:57.861130: step 630, total loss = 0.58, batch loss = 0.05 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:06s remains)
INFO - root - 2018-03-12 23:35:59.969574: step 640, total loss = 0.60, batch loss = 0.07 (43.1 examples/sec; 0.186 sec/batch; 17h:06m:45s remains)
INFO - root - 2018-03-12 23:36:02.151535: step 650, total loss = 0.58, batch loss = 0.05 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:21s remains)
INFO - root - 2018-03-12 23:36:04.273122: step 660, total loss = 0.58, batch loss = 0.05 (45.0 examples/sec; 0.178 sec/batch; 16h:22m:37s remains)
INFO - root - 2018-03-12 23:36:06.378835: step 670, total loss = 0.58, batch loss = 0.05 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:55s remains)
INFO - root - 2018-03-12 23:36:08.512106: step 680, total loss = 0.60, batch loss = 0.07 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:30s remains)
INFO - root - 2018-03-12 23:36:10.702744: step 690, total loss = 0.61, batch loss = 0.08 (39.5 examples/sec; 0.202 sec/batch; 18h:39m:31s remains)
INFO - root - 2018-03-12 23:36:12.813135: step 700, total loss = 0.57, batch loss = 0.04 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:54s remains)
INFO - root - 2018-03-12 23:36:15.099812: step 710, total loss = 0.58, batch loss = 0.05 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:27s remains)
INFO - root - 2018-03-12 23:36:17.249506: step 720, total loss = 0.58, batch loss = 0.05 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:25s remains)
INFO - root - 2018-03-12 23:36:19.358539: step 730, total loss = 0.58, batch loss = 0.06 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:15s remains)
INFO - root - 2018-03-12 23:36:21.515525: step 740, total loss = 0.57, batch loss = 0.04 (39.5 examples/sec; 0.203 sec/batch; 18h:40m:50s remains)
INFO - root - 2018-03-12 23:36:23.647538: step 750, total loss = 0.58, batch loss = 0.05 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:09s remains)
INFO - root - 2018-03-12 23:36:25.807621: step 760, total loss = 0.56, batch loss = 0.04 (40.0 examples/sec; 0.200 sec/batch; 18h:26m:44s remains)
INFO - root - 2018-03-12 23:36:27.962540: step 770, total loss = 0.57, batch loss = 0.04 (34.2 examples/sec; 0.234 sec/batch; 21h:32m:54s remains)
INFO - root - 2018-03-12 23:36:30.111563: step 780, total loss = 0.59, batch loss = 0.07 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:12s remains)
INFO - root - 2018-03-12 23:36:32.215050: step 790, total loss = 0.56, batch loss = 0.04 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:20s remains)
INFO - root - 2018-03-12 23:36:34.378717: step 800, total loss = 0.60, batch loss = 0.08 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:09s remains)
INFO - root - 2018-03-12 23:36:36.682668: step 810, total loss = 0.57, batch loss = 0.04 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:27s remains)
INFO - root - 2018-03-12 23:36:38.854863: step 820, total loss = 0.56, batch loss = 0.04 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:21s remains)
INFO - root - 2018-03-12 23:36:41.030196: step 830, total loss = 0.57, batch loss = 0.04 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:03s remains)
INFO - root - 2018-03-12 23:36:43.204379: step 840, total loss = 0.58, batch loss = 0.06 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:32s remains)
INFO - root - 2018-03-12 23:36:45.302209: step 850, total loss = 0.57, batch loss = 0.05 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:55s remains)
INFO - root - 2018-03-12 23:36:47.439102: step 860, total loss = 0.57, batch loss = 0.05 (40.5 examples/sec; 0.198 sec/batch; 18h:11m:40s remains)
INFO - root - 2018-03-12 23:36:49.556214: step 870, total loss = 0.56, batch loss = 0.04 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:40s remains)
INFO - root - 2018-03-12 23:36:51.696128: step 880, total loss = 0.61, batch loss = 0.09 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:26s remains)
INFO - root - 2018-03-12 23:36:53.862458: step 890, total loss = 0.56, batch loss = 0.04 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:04s remains)
INFO - root - 2018-03-12 23:36:55.972331: step 900, total loss = 0.58, batch loss = 0.06 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:53s remains)
INFO - root - 2018-03-12 23:36:58.195677: step 910, total loss = 0.57, batch loss = 0.05 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:57s remains)
INFO - root - 2018-03-12 23:37:00.324159: step 920, total loss = 0.58, batch loss = 0.06 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:55s remains)
INFO - root - 2018-03-12 23:37:02.465336: step 930, total loss = 0.56, batch loss = 0.04 (41.6 examples/sec; 0.192 sec/batch; 17h:43m:45s remains)
INFO - root - 2018-03-12 23:37:04.650145: step 940, total loss = 0.58, batch loss = 0.06 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:57s remains)
