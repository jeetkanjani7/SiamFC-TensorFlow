INFO - SiamFC-3s-gray-scratch - Running command 'main'
INFO - SiamFC-3s-gray-scratch - Started run with ID "7"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
INFO - root - preproces -- siamese_fc_gray
INFO - root - embedding init method -- kaiming_normal
INFO - root - preproces -- None
2018-03-13 23:50:25.574070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-13 23:50:25.673261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-13 23:50:25.673680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.30GiB
2018-03-13 23:50:25.673699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO - root - Train for 332500 steps
INFO - root - 2018-03-13 23:50:29.207711: step 0, total loss = 1.26, batch loss = 0.69 (2.6 examples/sec; 3.035 sec/batch; 280h:17m:00s remains)
INFO:tensorflow:Logs/SiamFC/track_model_checkpoints/SiamFC-3s-gray-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - Logs/SiamFC/track_model_checkpoints/SiamFC-3s-gray-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
Exempler Size bhenchod: 
Tensor("train/IteratorGetNext:0", shape=(?, 127, 127, 3), dtype=float32, device=/device:CPU:0)
Tensor("train/IteratorGetNext:1", shape=(?, 239, 239, 3), dtype=float32, device=/device:CPU:0)
NET SHAPE PREV >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 6, 6, 256)
6.0
NET SHAPE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 6, 6, 256)
NET SHAPE PREV >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 20, 20, 256)
20.0
NET SHAPE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 20, 20, 256)
Shapes bhenchod ...........
(1, 20, 20, 256)
(6, 6, 256, 1)
Gaand mariiiii..............
HEIGHT: [15, 15].....................
Exempler Size bhenchod: 
Tensor("validation/ToFloat:0", shape=(?, 127, 127, 3), dtype=float32, device=/device:CPU:0)
Tensor("validation/ToFloat_1:0", shape=(?, 255, 255, 3), dtype=float32, device=/device:CPU:0)
NET SHAPE PREV >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 6, 6, 256)
6.0
NET SHAPE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 6, 6, 256)
NET SHAPE PREV >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 22, 22, 256)
22.0
NET SHAPE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(?, 22, 22, 256)
Shapes bhenchod ...........
(1, 22, 22, 256)
(6, 6, 256, 1)
Gaand mariiiii..............
HEIGHT: [17, 17].....................
MODEL BUILT SUCCESSFULLY...................
EVERYTHING FINE TRAINING OPS.........................
INFO - root - 2018-03-13 23:50:36.575477: step 10, total loss = 1.26, batch loss = 0.69 (13.5 examples/sec; 0.592 sec/batch; 54h:40m:20s remains)
INFO - root - 2018-03-13 23:50:42.561897: step 20, total loss = 1.26, batch loss = 0.69 (13.5 examples/sec; 0.595 sec/batch; 54h:54m:52s remains)
INFO - root - 2018-03-13 23:50:48.697808: step 30, total loss = 1.26, batch loss = 0.69 (14.2 examples/sec; 0.564 sec/batch; 52h:07m:15s remains)
INFO - root - 2018-03-13 23:50:55.290041: step 40, total loss = 1.26, batch loss = 0.69 (13.6 examples/sec; 0.586 sec/batch; 54h:08m:23s remains)
INFO - root - 2018-03-13 23:51:01.445144: step 50, total loss = 1.26, batch loss = 0.69 (13.9 examples/sec; 0.576 sec/batch; 53h:09m:21s remains)
INFO - root - 2018-03-13 23:51:08.436890: step 60, total loss = 1.26, batch loss = 0.69 (12.9 examples/sec; 0.620 sec/batch; 57h:13m:53s remains)
INFO - root - 2018-03-13 23:51:14.490329: step 70, total loss = 1.25, batch loss = 0.69 (13.4 examples/sec; 0.599 sec/batch; 55h:16m:58s remains)
INFO - root - 2018-03-13 23:51:20.515091: step 80, total loss = 1.25, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:53m:46s remains)
INFO - root - 2018-03-13 23:51:26.507913: step 90, total loss = 1.25, batch loss = 0.69 (13.4 examples/sec; 0.597 sec/batch; 55h:08m:55s remains)
INFO - root - 2018-03-13 23:51:32.497368: step 100, total loss = 1.25, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:22m:13s remains)
INFO - root - 2018-03-13 23:51:39.226711: step 110, total loss = 1.25, batch loss = 0.69 (13.2 examples/sec; 0.604 sec/batch; 55h:46m:37s remains)
INFO - root - 2018-03-13 23:51:45.267918: step 120, total loss = 1.25, batch loss = 0.69 (13.5 examples/sec; 0.591 sec/batch; 54h:33m:38s remains)
INFO - root - 2018-03-13 23:51:51.281522: step 130, total loss = 1.25, batch loss = 0.69 (13.4 examples/sec; 0.599 sec/batch; 55h:16m:36s remains)
INFO - root - 2018-03-13 23:51:57.311700: step 140, total loss = 1.25, batch loss = 0.69 (14.1 examples/sec; 0.567 sec/batch; 52h:21m:38s remains)
INFO - root - 2018-03-13 23:52:03.316127: step 150, total loss = 1.25, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:12m:45s remains)
INFO - root - 2018-03-13 23:52:09.297697: step 160, total loss = 1.25, batch loss = 0.69 (13.6 examples/sec; 0.590 sec/batch; 54h:28m:18s remains)
INFO - root - 2018-03-13 23:52:15.575162: step 170, total loss = 1.25, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:49m:43s remains)
INFO - root - 2018-03-13 23:52:21.692258: step 180, total loss = 1.25, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 54h:48m:21s remains)
INFO - root - 2018-03-13 23:52:27.743037: step 190, total loss = 1.25, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:24m:22s remains)
INFO - root - 2018-03-13 23:52:33.754650: step 200, total loss = 1.25, batch loss = 0.69 (13.1 examples/sec; 0.609 sec/batch; 56h:14m:48s remains)
INFO - root - 2018-03-13 23:52:40.963649: step 210, total loss = 1.25, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 62h:10m:07s remains)
INFO - root - 2018-03-13 23:52:46.966311: step 220, total loss = 1.25, batch loss = 0.69 (13.2 examples/sec; 0.604 sec/batch; 55h:44m:50s remains)
INFO - root - 2018-03-13 23:52:52.957085: step 230, total loss = 1.25, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:55m:31s remains)
INFO - root - 2018-03-13 23:52:58.982729: step 240, total loss = 1.24, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:17m:57s remains)
INFO - root - 2018-03-13 23:53:04.972762: step 250, total loss = 1.24, batch loss = 0.69 (13.4 examples/sec; 0.597 sec/batch; 55h:04m:30s remains)
INFO - root - 2018-03-13 23:53:10.976739: step 260, total loss = 1.24, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 54h:48m:18s remains)
INFO - root - 2018-03-13 23:53:16.973826: step 270, total loss = 1.24, batch loss = 0.69 (13.3 examples/sec; 0.604 sec/batch; 55h:42m:52s remains)
INFO - root - 2018-03-13 23:53:22.985486: step 280, total loss = 1.24, batch loss = 0.69 (13.8 examples/sec; 0.578 sec/batch; 53h:20m:20s remains)
INFO - root - 2018-03-13 23:53:28.996435: step 290, total loss = 1.24, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:33m:55s remains)
INFO - root - 2018-03-13 23:53:35.036389: step 300, total loss = 1.24, batch loss = 0.69 (13.1 examples/sec; 0.612 sec/batch; 56h:29m:27s remains)
INFO - root - 2018-03-13 23:53:41.743084: step 310, total loss = 1.24, batch loss = 0.69 (13.5 examples/sec; 0.593 sec/batch; 54h:44m:07s remains)
INFO - root - 2018-03-13 23:53:47.792352: step 320, total loss = 1.24, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 59h:03m:28s remains)
INFO - root - 2018-03-13 23:53:53.992533: step 330, total loss = 1.24, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:44m:54s remains)
INFO - root - 2018-03-13 23:54:00.201562: step 340, total loss = 1.24, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 59h:03m:24s remains)
INFO - root - 2018-03-13 23:54:06.224075: step 350, total loss = 1.24, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 55h:57m:55s remains)
INFO - root - 2018-03-13 23:54:12.223815: step 360, total loss = 1.24, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:28m:52s remains)
INFO - root - 2018-03-13 23:54:18.194241: step 370, total loss = 1.24, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:32m:11s remains)
INFO - root - 2018-03-13 23:54:24.227138: step 380, total loss = 1.24, batch loss = 0.69 (13.6 examples/sec; 0.590 sec/batch; 54h:23m:14s remains)
INFO - root - 2018-03-13 23:54:30.233179: step 390, total loss = 1.24, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:14m:05s remains)
INFO - root - 2018-03-13 23:54:36.181524: step 400, total loss = 1.23, batch loss = 0.69 (13.5 examples/sec; 0.592 sec/batch; 54h:38m:24s remains)
INFO - root - 2018-03-13 23:54:43.382605: step 410, total loss = 1.24, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:54m:57s remains)
INFO - root - 2018-03-13 23:54:49.442667: step 420, total loss = 1.23, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:29m:43s remains)
INFO - root - 2018-03-13 23:54:55.450172: step 430, total loss = 1.23, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:52m:27s remains)
INFO - root - 2018-03-13 23:55:01.419169: step 440, total loss = 1.23, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:46m:12s remains)
INFO - root - 2018-03-13 23:55:07.461248: step 450, total loss = 1.23, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 56h:00m:27s remains)
INFO - root - 2018-03-13 23:55:13.455321: step 460, total loss = 1.23, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 57h:00m:28s remains)
INFO - root - 2018-03-13 23:55:19.454089: step 470, total loss = 1.23, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:27m:31s remains)
INFO - root - 2018-03-13 23:55:25.770460: step 480, total loss = 1.23, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:38m:34s remains)
INFO - root - 2018-03-13 23:55:32.324653: step 490, total loss = 1.23, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:45m:50s remains)
INFO - root - 2018-03-13 23:55:38.424536: step 500, total loss = 1.23, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:33m:33s remains)
INFO - root - 2018-03-13 23:55:45.406015: step 510, total loss = 1.23, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:14m:47s remains)
INFO - root - 2018-03-13 23:55:51.436179: step 520, total loss = 1.23, batch loss = 0.69 (13.6 examples/sec; 0.588 sec/batch; 54h:15m:06s remains)
INFO - root - 2018-03-13 23:55:57.461516: step 530, total loss = 1.23, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:17m:38s remains)
INFO - root - 2018-03-13 23:56:03.476767: step 540, total loss = 1.22, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:23m:19s remains)
INFO - root - 2018-03-13 23:56:09.480674: step 550, total loss = 1.22, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:27m:08s remains)
INFO - root - 2018-03-13 23:56:15.471461: step 560, total loss = 1.22, batch loss = 0.69 (13.9 examples/sec; 0.575 sec/batch; 52h:59m:57s remains)
INFO - root - 2018-03-13 23:56:21.514504: step 570, total loss = 1.22, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:25m:51s remains)
INFO - root - 2018-03-13 23:56:27.724848: step 580, total loss = 1.22, batch loss = 0.69 (11.2 examples/sec; 0.712 sec/batch; 65h:36m:54s remains)
INFO - root - 2018-03-13 23:56:34.527794: step 590, total loss = 1.22, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:23m:46s remains)
INFO - root - 2018-03-13 23:56:41.184832: step 600, total loss = 1.22, batch loss = 0.68 (7.9 examples/sec; 1.010 sec/batch; 93h:04m:24s remains)
INFO - root - 2018-03-13 23:56:49.215049: step 610, total loss = 1.22, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:55m:53s remains)
INFO - root - 2018-03-13 23:56:55.871912: step 620, total loss = 1.22, batch loss = 0.69 (13.0 examples/sec; 0.618 sec/batch; 56h:56m:18s remains)
INFO - root - 2018-03-13 23:57:02.229617: step 630, total loss = 1.23, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:52m:37s remains)
INFO - root - 2018-03-13 23:57:08.646752: step 640, total loss = 1.21, batch loss = 0.68 (12.2 examples/sec; 0.656 sec/batch; 60h:27m:03s remains)
INFO - root - 2018-03-13 23:57:15.059671: step 650, total loss = 1.22, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:35m:25s remains)
INFO - root - 2018-03-13 23:57:21.523217: step 660, total loss = 1.21, batch loss = 0.68 (12.3 examples/sec; 0.652 sec/batch; 60h:07m:36s remains)
INFO - root - 2018-03-13 23:57:27.956042: step 670, total loss = 1.21, batch loss = 0.68 (12.3 examples/sec; 0.652 sec/batch; 60h:03m:15s remains)
INFO - root - 2018-03-13 23:57:34.303962: step 680, total loss = 1.21, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 58h:03m:32s remains)
INFO - root - 2018-03-13 23:57:40.708255: step 690, total loss = 1.20, batch loss = 0.68 (11.9 examples/sec; 0.675 sec/batch; 62h:12m:05s remains)
INFO - root - 2018-03-13 23:57:46.924850: step 700, total loss = 1.21, batch loss = 0.68 (12.4 examples/sec; 0.646 sec/batch; 59h:31m:10s remains)
INFO - root - 2018-03-13 23:57:54.402002: step 710, total loss = 1.21, batch loss = 0.68 (13.1 examples/sec; 0.610 sec/batch; 56h:14m:07s remains)
INFO - root - 2018-03-13 23:58:00.914673: step 720, total loss = 1.21, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 59h:07m:55s remains)
INFO - root - 2018-03-13 23:58:07.283665: step 730, total loss = 1.21, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 59h:01m:51s remains)
INFO - root - 2018-03-13 23:58:13.781012: step 740, total loss = 1.20, batch loss = 0.68 (12.0 examples/sec; 0.666 sec/batch; 61h:21m:53s remains)
INFO - root - 2018-03-13 23:58:20.343757: step 750, total loss = 1.21, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 59h:08m:56s remains)
INFO - root - 2018-03-13 23:58:26.884798: step 760, total loss = 1.21, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:54m:25s remains)
INFO - root - 2018-03-13 23:58:33.496719: step 770, total loss = 1.20, batch loss = 0.68 (12.2 examples/sec; 0.658 sec/batch; 60h:40m:14s remains)
INFO - root - 2018-03-13 23:58:40.062415: step 780, total loss = 1.21, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 58h:10m:46s remains)
INFO - root - 2018-03-13 23:58:46.546798: step 790, total loss = 1.20, batch loss = 0.68 (12.6 examples/sec; 0.637 sec/batch; 58h:42m:33s remains)
INFO - root - 2018-03-13 23:58:53.226334: step 800, total loss = 1.20, batch loss = 0.68 (12.1 examples/sec; 0.661 sec/batch; 60h:53m:23s remains)
INFO - root - 2018-03-13 23:59:00.317270: step 810, total loss = 1.21, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:21m:12s remains)
INFO - root - 2018-03-13 23:59:06.932372: step 820, total loss = 1.19, batch loss = 0.67 (12.3 examples/sec; 0.651 sec/batch; 60h:01m:27s remains)
INFO - root - 2018-03-13 23:59:13.321388: step 830, total loss = 1.20, batch loss = 0.68 (12.5 examples/sec; 0.638 sec/batch; 58h:46m:03s remains)
INFO - root - 2018-03-13 23:59:19.806211: step 840, total loss = 1.20, batch loss = 0.67 (12.1 examples/sec; 0.662 sec/batch; 60h:58m:08s remains)
INFO - root - 2018-03-13 23:59:26.303453: step 850, total loss = 1.19, batch loss = 0.67 (12.4 examples/sec; 0.648 sec/batch; 59h:39m:45s remains)
INFO - root - 2018-03-13 23:59:32.713794: step 860, total loss = 1.20, batch loss = 0.68 (12.5 examples/sec; 0.638 sec/batch; 58h:45m:43s remains)
INFO - root - 2018-03-13 23:59:39.191627: step 870, total loss = 1.19, batch loss = 0.67 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:01s remains)
INFO - root - 2018-03-13 23:59:45.641050: step 880, total loss = 1.20, batch loss = 0.68 (12.4 examples/sec; 0.645 sec/batch; 59h:24m:28s remains)
INFO - root - 2018-03-13 23:59:52.025320: step 890, total loss = 1.19, batch loss = 0.67 (12.2 examples/sec; 0.654 sec/batch; 60h:12m:01s remains)
INFO - root - 2018-03-13 23:59:58.297929: step 900, total loss = 1.18, batch loss = 0.66 (12.7 examples/sec; 0.631 sec/batch; 58h:09m:25s remains)
INFO - root - 2018-03-14 00:00:05.379207: step 910, total loss = 1.19, batch loss = 0.67 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:47s remains)
INFO - root - 2018-03-14 00:00:11.861235: step 920, total loss = 1.21, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:33m:47s remains)
INFO - root - 2018-03-14 00:00:18.475998: step 930, total loss = 1.18, batch loss = 0.67 (12.0 examples/sec; 0.669 sec/batch; 61h:38m:30s remains)
INFO - root - 2018-03-14 00:00:25.001332: step 940, total loss = 1.19, batch loss = 0.67 (11.8 examples/sec; 0.678 sec/batch; 62h:28m:41s remains)
INFO - root - 2018-03-14 00:00:31.724078: step 950, total loss = 1.18, batch loss = 0.66 (9.7 examples/sec; 0.824 sec/batch; 75h:54m:07s remains)
INFO - root - 2018-03-14 00:00:38.309259: step 960, total loss = 1.18, batch loss = 0.67 (12.1 examples/sec; 0.659 sec/batch; 60h:43m:40s remains)
INFO - root - 2018-03-14 00:00:44.775390: step 970, total loss = 1.20, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:06m:47s remains)
INFO - root - 2018-03-14 00:00:51.248359: step 980, total loss = 1.20, batch loss = 0.68 (12.5 examples/sec; 0.638 sec/batch; 58h:43m:02s remains)
INFO - root - 2018-03-14 00:00:57.759605: step 990, total loss = 1.18, batch loss = 0.66 (12.9 examples/sec; 0.622 sec/batch; 57h:14m:11s remains)
INFO - root - 2018-03-14 00:01:04.270694: step 1000, total loss = 1.19, batch loss = 0.67 (12.6 examples/sec; 0.635 sec/batch; 58h:30m:05s remains)
INFO - root - 2018-03-14 00:01:11.275192: step 1010, total loss = 1.18, batch loss = 0.67 (12.4 examples/sec; 0.645 sec/batch; 59h:23m:55s remains)
INFO - root - 2018-03-14 00:01:17.690882: step 1020, total loss = 1.17, batch loss = 0.66 (12.7 examples/sec; 0.628 sec/batch; 57h:48m:43s remains)
INFO - root - 2018-03-14 00:01:24.117601: step 1030, total loss = 1.17, batch loss = 0.66 (12.8 examples/sec; 0.624 sec/batch; 57h:26m:03s remains)
INFO - root - 2018-03-14 00:01:30.591853: step 1040, total loss = 1.20, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:26m:30s remains)
INFO - root - 2018-03-14 00:01:37.032838: step 1050, total loss = 1.15, batch loss = 0.64 (12.6 examples/sec; 0.634 sec/batch; 58h:23m:25s remains)
INFO - root - 2018-03-14 00:01:43.533108: step 1060, total loss = 1.16, batch loss = 0.65 (12.5 examples/sec; 0.638 sec/batch; 58h:46m:30s remains)
INFO - root - 2018-03-14 00:01:49.974949: step 1070, total loss = 1.16, batch loss = 0.65 (11.9 examples/sec; 0.672 sec/batch; 61h:51m:40s remains)
INFO - root - 2018-03-14 00:01:56.357077: step 1080, total loss = 1.15, batch loss = 0.64 (12.3 examples/sec; 0.650 sec/batch; 59h:51m:14s remains)
INFO - root - 2018-03-14 00:02:02.845959: step 1090, total loss = 1.14, batch loss = 0.63 (12.1 examples/sec; 0.659 sec/batch; 60h:41m:04s remains)
INFO - root - 2018-03-14 00:02:09.266077: step 1100, total loss = 1.14, batch loss = 0.63 (12.9 examples/sec; 0.620 sec/batch; 57h:04m:38s remains)
INFO - root - 2018-03-14 00:02:16.270032: step 1110, total loss = 1.15, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 59h:08m:36s remains)
INFO - root - 2018-03-14 00:02:22.684736: step 1120, total loss = 1.15, batch loss = 0.65 (12.3 examples/sec; 0.651 sec/batch; 59h:53m:41s remains)
INFO - root - 2018-03-14 00:02:29.159749: step 1130, total loss = 1.15, batch loss = 0.65 (12.6 examples/sec; 0.636 sec/batch; 58h:33m:13s remains)
INFO - root - 2018-03-14 00:02:35.565998: step 1140, total loss = 1.15, batch loss = 0.64 (12.4 examples/sec; 0.646 sec/batch; 59h:25m:41s remains)
INFO - root - 2018-03-14 00:02:41.996642: step 1150, total loss = 1.13, batch loss = 0.62 (12.7 examples/sec; 0.629 sec/batch; 57h:52m:58s remains)
INFO - root - 2018-03-14 00:02:48.418155: step 1160, total loss = 1.13, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 59h:04m:10s remains)
INFO - root - 2018-03-14 00:02:54.787082: step 1170, total loss = 1.14, batch loss = 0.64 (12.8 examples/sec; 0.626 sec/batch; 57h:36m:38s remains)
INFO - root - 2018-03-14 00:03:01.213207: step 1180, total loss = 1.12, batch loss = 0.62 (12.5 examples/sec; 0.641 sec/batch; 58h:59m:10s remains)
INFO - root - 2018-03-14 00:03:07.621087: step 1190, total loss = 1.10, batch loss = 0.60 (12.8 examples/sec; 0.625 sec/batch; 57h:32m:24s remains)
INFO - root - 2018-03-14 00:03:14.032811: step 1200, total loss = 1.11, batch loss = 0.61 (12.6 examples/sec; 0.634 sec/batch; 58h:20m:32s remains)
INFO - root - 2018-03-14 00:03:21.175943: step 1210, total loss = 1.12, batch loss = 0.62 (12.5 examples/sec; 0.640 sec/batch; 58h:52m:24s remains)
INFO - root - 2018-03-14 00:03:27.523648: step 1220, total loss = 1.10, batch loss = 0.60 (12.7 examples/sec; 0.628 sec/batch; 57h:45m:05s remains)
INFO - root - 2018-03-14 00:03:33.920417: step 1230, total loss = 1.08, batch loss = 0.58 (12.3 examples/sec; 0.650 sec/batch; 59h:47m:16s remains)
INFO - root - 2018-03-14 00:03:40.302048: step 1240, total loss = 1.10, batch loss = 0.60 (13.4 examples/sec; 0.597 sec/batch; 54h:55m:02s remains)
INFO - root - 2018-03-14 00:03:46.656450: step 1250, total loss = 1.10, batch loss = 0.60 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:47s remains)
INFO - root - 2018-03-14 00:03:53.098454: step 1260, total loss = 1.08, batch loss = 0.58 (12.2 examples/sec; 0.656 sec/batch; 60h:18m:55s remains)
INFO - root - 2018-03-14 00:03:59.453483: step 1270, total loss = 1.05, batch loss = 0.55 (12.3 examples/sec; 0.653 sec/batch; 60h:02m:45s remains)
INFO - root - 2018-03-14 00:04:05.823065: step 1280, total loss = 1.13, batch loss = 0.63 (13.2 examples/sec; 0.606 sec/batch; 55h:44m:21s remains)
INFO - root - 2018-03-14 00:04:12.224858: step 1290, total loss = 1.08, batch loss = 0.58 (12.2 examples/sec; 0.658 sec/batch; 60h:32m:37s remains)
INFO - root - 2018-03-14 00:04:18.592289: step 1300, total loss = 1.08, batch loss = 0.59 (12.2 examples/sec; 0.653 sec/batch; 60h:05m:46s remains)
INFO - root - 2018-03-14 00:04:25.642236: step 1310, total loss = 1.13, batch loss = 0.63 (13.0 examples/sec; 0.614 sec/batch; 56h:29m:37s remains)
INFO - root - 2018-03-14 00:04:32.026834: step 1320, total loss = 1.07, batch loss = 0.57 (12.8 examples/sec; 0.625 sec/batch; 57h:27m:02s remains)
INFO - root - 2018-03-14 00:04:38.308469: step 1330, total loss = 1.02, batch loss = 0.52 (12.8 examples/sec; 0.627 sec/batch; 57h:38m:31s remains)
INFO - root - 2018-03-14 00:04:44.772226: step 1340, total loss = 1.02, batch loss = 0.52 (12.5 examples/sec; 0.642 sec/batch; 59h:02m:07s remains)
INFO - root - 2018-03-14 00:04:51.106281: step 1350, total loss = 1.08, batch loss = 0.59 (12.2 examples/sec; 0.654 sec/batch; 60h:07m:24s remains)
INFO - root - 2018-03-14 00:04:57.420905: step 1360, total loss = 1.08, batch loss = 0.59 (13.2 examples/sec; 0.608 sec/batch; 55h:55m:03s remains)
INFO - root - 2018-03-14 00:05:03.877330: step 1370, total loss = 1.00, batch loss = 0.50 (12.4 examples/sec; 0.644 sec/batch; 59h:16m:17s remains)
INFO - root - 2018-03-14 00:05:10.130417: step 1380, total loss = 1.02, batch loss = 0.53 (12.7 examples/sec; 0.630 sec/batch; 57h:57m:26s remains)
INFO - root - 2018-03-14 00:05:16.561653: step 1390, total loss = 1.03, batch loss = 0.53 (12.3 examples/sec; 0.651 sec/batch; 59h:52m:21s remains)
INFO - root - 2018-03-14 00:05:22.967707: step 1400, total loss = 1.03, batch loss = 0.54 (12.6 examples/sec; 0.637 sec/batch; 58h:35m:27s remains)
INFO - root - 2018-03-14 00:05:29.944367: step 1410, total loss = 1.04, batch loss = 0.55 (12.6 examples/sec; 0.635 sec/batch; 58h:26m:18s remains)
INFO - root - 2018-03-14 00:05:36.408824: step 1420, total loss = 0.99, batch loss = 0.50 (12.6 examples/sec; 0.633 sec/batch; 58h:12m:30s remains)
INFO - root - 2018-03-14 00:05:42.831890: step 1430, total loss = 0.97, batch loss = 0.48 (12.9 examples/sec; 0.620 sec/batch; 57h:00m:41s remains)
INFO - root - 2018-03-14 00:05:49.151295: step 1440, total loss = 1.01, batch loss = 0.52 (12.9 examples/sec; 0.622 sec/batch; 57h:12m:59s remains)
INFO - root - 2018-03-14 00:05:55.539833: step 1450, total loss = 0.97, batch loss = 0.48 (13.2 examples/sec; 0.606 sec/batch; 55h:42m:35s remains)
INFO - root - 2018-03-14 00:06:01.845658: step 1460, total loss = 0.97, batch loss = 0.48 (12.7 examples/sec; 0.629 sec/batch; 57h:49m:28s remains)
INFO - root - 2018-03-14 00:06:08.242196: step 1470, total loss = 0.96, batch loss = 0.47 (12.5 examples/sec; 0.639 sec/batch; 58h:45m:12s remains)
INFO - root - 2018-03-14 00:06:14.573472: step 1480, total loss = 0.96, batch loss = 0.47 (12.3 examples/sec; 0.648 sec/batch; 59h:36m:47s remains)
INFO - root - 2018-03-14 00:06:20.917025: step 1490, total loss = 0.94, batch loss = 0.45 (12.8 examples/sec; 0.623 sec/batch; 57h:19m:42s remains)
INFO - root - 2018-03-14 00:06:27.342773: step 1500, total loss = 0.95, batch loss = 0.46 (13.0 examples/sec; 0.616 sec/batch; 56h:38m:15s remains)
INFO - root - 2018-03-14 00:06:34.520943: step 1510, total loss = 0.95, batch loss = 0.46 (12.5 examples/sec; 0.642 sec/batch; 59h:03m:40s remains)
INFO - root - 2018-03-14 00:06:41.001735: step 1520, total loss = 0.92, batch loss = 0.43 (12.2 examples/sec; 0.657 sec/batch; 60h:24m:18s remains)
INFO - root - 2018-03-14 00:06:47.346167: step 1530, total loss = 0.96, batch loss = 0.47 (12.7 examples/sec; 0.632 sec/batch; 58h:04m:28s remains)
INFO - root - 2018-03-14 00:06:53.806933: step 1540, total loss = 0.95, batch loss = 0.46 (12.3 examples/sec; 0.653 sec/batch; 59h:59m:33s remains)
INFO - root - 2018-03-14 00:07:00.101288: step 1550, total loss = 0.96, batch loss = 0.47 (12.2 examples/sec; 0.658 sec/batch; 60h:26m:45s remains)
INFO - root - 2018-03-14 00:07:06.480536: step 1560, total loss = 0.92, batch loss = 0.43 (12.3 examples/sec; 0.651 sec/batch; 59h:52m:09s remains)
INFO - root - 2018-03-14 00:07:12.838178: step 1570, total loss = 0.98, batch loss = 0.49 (12.2 examples/sec; 0.655 sec/batch; 60h:13m:14s remains)
INFO - root - 2018-03-14 00:07:19.112770: step 1580, total loss = 0.90, batch loss = 0.42 (13.1 examples/sec; 0.610 sec/batch; 56h:05m:36s remains)
INFO - root - 2018-03-14 00:07:25.527854: step 1590, total loss = 0.97, batch loss = 0.48 (12.3 examples/sec; 0.653 sec/batch; 60h:01m:36s remains)
INFO - root - 2018-03-14 00:07:31.798667: step 1600, total loss = 0.90, batch loss = 0.41 (12.7 examples/sec; 0.629 sec/batch; 57h:51m:12s remains)
INFO - root - 2018-03-14 00:07:38.794742: step 1610, total loss = 0.96, batch loss = 0.48 (13.0 examples/sec; 0.615 sec/batch; 56h:33m:36s remains)
INFO - root - 2018-03-14 00:07:45.157926: step 1620, total loss = 0.92, batch loss = 0.44 (12.2 examples/sec; 0.654 sec/batch; 60h:06m:02s remains)
INFO - root - 2018-03-14 00:07:51.509776: step 1630, total loss = 0.96, batch loss = 0.48 (13.6 examples/sec; 0.589 sec/batch; 54h:08m:24s remains)
INFO - root - 2018-03-14 00:07:57.912568: step 1640, total loss = 0.91, batch loss = 0.43 (12.3 examples/sec; 0.651 sec/batch; 59h:51m:47s remains)
INFO - root - 2018-03-14 00:08:04.216347: step 1650, total loss = 0.96, batch loss = 0.48 (12.7 examples/sec; 0.630 sec/batch; 57h:56m:39s remains)
INFO - root - 2018-03-14 00:08:10.631569: step 1660, total loss = 0.94, batch loss = 0.46 (12.7 examples/sec; 0.629 sec/batch; 57h:50m:36s remains)
INFO - root - 2018-03-14 00:08:16.896402: step 1670, total loss = 1.01, batch loss = 0.53 (12.1 examples/sec; 0.663 sec/batch; 60h:54m:15s remains)
INFO - root - 2018-03-14 00:08:24.202678: step 1680, total loss = 0.89, batch loss = 0.41 (11.4 examples/sec; 0.702 sec/batch; 64h:28m:02s remains)
INFO - root - 2018-03-14 00:08:30.916908: step 1690, total loss = 0.86, batch loss = 0.38 (12.4 examples/sec; 0.645 sec/batch; 59h:14m:30s remains)
INFO - root - 2018-03-14 00:08:37.865547: step 1700, total loss = 0.92, batch loss = 0.44 (12.4 examples/sec; 0.646 sec/batch; 59h:21m:33s remains)
INFO - root - 2018-03-14 00:08:45.276616: step 1710, total loss = 0.93, batch loss = 0.45 (12.5 examples/sec; 0.638 sec/batch; 58h:35m:30s remains)
INFO - root - 2018-03-14 00:08:52.002104: step 1720, total loss = 0.90, batch loss = 0.42 (12.3 examples/sec; 0.651 sec/batch; 59h:47m:02s remains)
INFO - root - 2018-03-14 00:08:58.455958: step 1730, total loss = 0.88, batch loss = 0.40 (12.6 examples/sec; 0.632 sec/batch; 58h:06m:23s remains)
INFO - root - 2018-03-14 00:09:04.864606: step 1740, total loss = 0.91, batch loss = 0.43 (12.8 examples/sec; 0.624 sec/batch; 57h:20m:10s remains)
INFO - root - 2018-03-14 00:09:11.367355: step 1750, total loss = 0.88, batch loss = 0.41 (12.3 examples/sec; 0.648 sec/batch; 59h:31m:54s remains)
INFO - root - 2018-03-14 00:09:17.843125: step 1760, total loss = 0.84, batch loss = 0.36 (12.4 examples/sec; 0.645 sec/batch; 59h:14m:55s remains)
INFO - root - 2018-03-14 00:09:24.243248: step 1770, total loss = 0.93, batch loss = 0.45 (12.6 examples/sec; 0.637 sec/batch; 58h:32m:34s remains)
INFO - root - 2018-03-14 00:09:30.750108: step 1780, total loss = 0.93, batch loss = 0.46 (12.6 examples/sec; 0.635 sec/batch; 58h:20m:34s remains)
INFO - root - 2018-03-14 00:09:37.234338: step 1790, total loss = 0.87, batch loss = 0.40 (12.5 examples/sec; 0.642 sec/batch; 58h:59m:32s remains)
INFO - root - 2018-03-14 00:09:43.647813: step 1800, total loss = 0.84, batch loss = 0.37 (12.4 examples/sec; 0.648 sec/batch; 59h:29m:23s remains)
INFO - root - 2018-03-14 00:09:50.885184: step 1810, total loss = 0.84, batch loss = 0.37 (12.3 examples/sec; 0.651 sec/batch; 59h:50m:43s remains)
INFO - root - 2018-03-14 00:09:57.257372: step 1820, total loss = 1.02, batch loss = 0.54 (12.8 examples/sec; 0.626 sec/batch; 57h:29m:15s remains)
INFO - root - 2018-03-14 00:10:03.723576: step 1830, total loss = 0.85, batch loss = 0.37 (12.1 examples/sec; 0.661 sec/batch; 60h:45m:11s remains)
INFO - root - 2018-03-14 00:10:10.145555: step 1840, total loss = 0.91, batch loss = 0.43 (12.6 examples/sec; 0.635 sec/batch; 58h:18m:18s remains)
INFO - root - 2018-03-14 00:10:16.572369: step 1850, total loss = 0.84, batch loss = 0.37 (12.6 examples/sec; 0.635 sec/batch; 58h:21m:02s remains)
